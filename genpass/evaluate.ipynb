{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YapingWu/GoogleColab/blob/main/genpass/evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6Fs_U9bt2h"
      },
      "source": [
        "说明：\r\n",
        "\r\n",
        "1.   上传tokenizer和lstm模型\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdIlDKhOXsJ"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import pickle\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import math"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PEJDfB3Geo7"
      },
      "source": [
        "# PL模型评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_A_LxZdRXPf"
      },
      "source": [
        "max_lengths = {\r\n",
        "    'myspace': 35,\r\n",
        "    'phpbb': 21,\r\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHwLahdqROFb"
      },
      "source": [
        "def sample(preds, temperature=1.0):\r\n",
        "  index = np.random.choice(np.arange(len(preds)), p=preds)\r\n",
        "  prop = preds[index]\r\n",
        "  return [index, prop]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIUJNW-nQxQP"
      },
      "source": [
        "def get_preds(pl_model, seed_text_list):\r\n",
        "  encoded = tokenizer.texts_to_sequences(seed_text_list)  # 对初始文本进行编码\r\n",
        "  encoded = pad_sequences(encoded, maxlen=max_length-1, padding='pre')\r\n",
        "  preds = pl_model.predict(encoded, batch_size=128)\r\n",
        "\r\n",
        "  rst_list = [sample(pred) for pred in preds]\r\n",
        "  result = pd.DataFrame(rst_list, columns=['index', 'prop'])\r\n",
        "  result['unit'] = result['index'].apply(lambda x: tokenizer.index_word[x].upper())\r\n",
        "\r\n",
        "  return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IMGnNw7Z8Jy"
      },
      "source": [
        "def generating(pl_model, filename, cnt=100):\r\n",
        "    # 使用模型生成密码\r\n",
        "    print('使用PL模型生成{:,}个wordlist：'.format(cnt))\r\n",
        "    units = [''] * cnt\r\n",
        "    gen_word_list = []\r\n",
        "    while cnt > 0:\r\n",
        "        preds = get_preds(pl_model, units)\r\n",
        "        concat_unit = np.char.add(units, [' '] * cnt)\r\n",
        "        concat_unit = np.char.add(concat_unit, preds['unit'])\r\n",
        "\r\n",
        "        new_units = []\r\n",
        "        for unit in concat_unit:\r\n",
        "            if '<END>' in unit:\r\n",
        "                gen_word_list.append(unit)\r\n",
        "            else:\r\n",
        "                new_units.append(unit)\r\n",
        "\r\n",
        "        # print('{:,}'.format(len(gen_word_list)))\r\n",
        "        units = new_units\r\n",
        "        cnt = len(units)\r\n",
        "\r\n",
        "    gen_word_list = np.char.replace(gen_word_list, '<END>', '')  # 删除末尾的'<END>'\r\n",
        "    gen_word_list = np.char.strip(gen_word_list)  # 去除前后空格\r\n",
        "\r\n",
        "    filename = '{}{}'.format('./', filename)\r\n",
        "    print('生成好的wordlist保存到文件：{}'.format(filename))\r\n",
        "    np.savetxt(filename, gen_word_list, fmt='%s', delimiter='\\n')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S4cF_JzGrEs"
      },
      "source": [
        "## one-site test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_VT1c8G9gu"
      },
      "source": [
        "### myspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bm7l20DcKMi"
      },
      "source": [
        "data_name = 'myspace'\r\n",
        "with open('/content/{}.pkl'.format(data_name), 'rb') as file:\r\n",
        "    tokenizer = pickle.load(file)\r\n",
        "max_length = max_lengths[data_name]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeTEbeusexpn"
      },
      "source": [
        "# 测试 get_preds\r\n",
        "# pl_model = load_model('/content/{}.h5'.format(data_name))\r\n",
        "# units = [''] * 100\r\n",
        "# preds = get_preds(pl_model, units)\r\n",
        "# print(preds.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiO29UU5GoTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40db4e6-bc07-4b6c-8a7e-6cd15632e68f"
      },
      "source": [
        "pl_model = load_model('/content/{}.h5'.format(data_name))  # 使用全部数据训练的模型\r\n",
        "for alpha in [3, 4, 5, 6, 7, 8, 9]:\r\n",
        "    generating(pl_model, '{}{}.txt'.format(data_name, alpha), cnt=int(math.pow(10, alpha)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用PL模型生成1,000个wordlist：\n",
            "生成好的wordlist保存到文件：./myspace3.txt\n",
            "使用PL模型生成10,000个wordlist：\n",
            "生成好的wordlist保存到文件：./myspace4.txt\n",
            "使用PL模型生成100,000个wordlist：\n",
            "生成好的wordlist保存到文件：./myspace5.txt\n",
            "使用PL模型生成1,000,000个wordlist：\n",
            "生成好的wordlist保存到文件：./myspace6.txt\n",
            "使用PL模型生成10,000,000个wordlist：\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PkZPlS9bkUf"
      },
      "source": [
        "pl_model = load_model('/content/{}_part.h5'.format('data_name'))  # 使用部分数据训练的模型\r\n",
        "for alpha in [3, 4, 5, 6, 7, 8, 9]:\r\n",
        "    generating(pl_model, '{}_{}_part.txt'.format(data_name, alpha), cnt=int(math.pow(10, alpha)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P40errNxHA21"
      },
      "source": [
        "### phpbb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBkKsdGcOep"
      },
      "source": [
        "data_name = 'myspace'\r\n",
        "with open('/content/{}.pkl'.format(data_name), 'rb') as file:\r\n",
        "    tokenizer = pickle.load(file)\r\n",
        "max_length = max_lengths[data_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA8VqAdwcQkQ"
      },
      "source": [
        "pl_model = load_model('/content/{}.h5'.format(data_name))  # 使用全部数据训练的模型\r\n",
        "for alpha in [3, 4, 5, 6, 7, 8, 9]:\r\n",
        "    generating(pl_model, '{}{}.txt'.format(data_name, alpha), cnt=int(math.pow(10, alpha)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvpvfeppcQ9E"
      },
      "source": [
        "pl_model = load_model('/content/{}_part.h5'.format('data_name'))  # 使用部分数据训练的模型\r\n",
        "for alpha in [3, 4, 5, 6, 7, 8, 9]:\r\n",
        "    generating(pl_model, '{}_{}_part.txt'.format(data_name, alpha), cnt=int(math.pow(10, alpha)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWOhU-0UGt-G"
      },
      "source": [
        "## cross-site test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkZLwYViG6uA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HciWHeC0GxWU"
      },
      "source": [
        "# GenPass模型评估"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6itpFODLI1Mj"
      },
      "source": [
        "# 其他命令"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJVm9xFZI4Wz"
      },
      "source": [
        "## 解压文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al0l6kSpI3fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f20647-3e96-4f10-b81e-14e0905c75e3"
      },
      "source": [
        "!unzip '/content/lstm_1.zip'\r\n",
        "!unzip '/content/tokenizer.zip'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/lstm_1.zip\n",
            "  inflating: phpbb.h5                \n",
            "  inflating: phpbb_part.h5           \n",
            "  inflating: myspace.h5              \n",
            "  inflating: myspace_part.h5         \n",
            "Archive:  /content/tokenizer.zip\n",
            "  inflating: phpbb.pkl               \n",
            "  inflating: rockyou.pkl             \n",
            "  inflating: myspace.pkl             \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}