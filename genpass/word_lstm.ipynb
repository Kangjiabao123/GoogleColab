{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBsRVYnrmLtkpurMZkfILe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YapingWu/GoogleColab/blob/main/genpass/word_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuLbXO_oxr1j"
      },
      "source": [
        "# 准备工作\r\n",
        "1. 上传编码后的密码文件：`myspace.txt` `phpbb.txt`\r\n",
        "2. 上传分词器模型：`myspace.pkl` `phpbb.pkl`\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaJUC-DFxb-r"
      },
      "source": [
        "# WordLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LYC6j5I_ufn"
      },
      "source": [
        "## 定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpNehT6sw8YN"
      },
      "source": [
        "from collections import ChainMap\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import time\r\n",
        "import pickle\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras import optimizers\r\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "SEED = 7\r\n",
        "max_seq_len = {\r\n",
        "        'myspace': 18,\r\n",
        "        'phpbb': 20,\r\n",
        "        'rockyou': 47,\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "class WordLSTM:\r\n",
        "    \"\"\"\r\n",
        "    Word-level LSTM模型\r\n",
        "\r\n",
        "    :param data_path: 数据集路径\r\n",
        "    :param data_name: 数据集名称\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, data_path, data_name):\r\n",
        "        self.data_name = data_name  # 数据集名称\r\n",
        "        self.data = self.load_data('{}{}.txt'.format(data_path, data_name))\r\n",
        "        # 分词器\r\n",
        "        self.tokenizer = self.load_tokenizer('{}{}.pkl'.format('/content/', data_name))\r\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1  # 词汇表大小\r\n",
        "\r\n",
        "        self.max_length = max_seq_len[data_name]  # 最大序列长度（单词长度）\r\n",
        "        # lstm的超参数\r\n",
        "        self.epochs = 200\r\n",
        "        self.batch_size = 128\r\n",
        "        self.lstm_layers = [[32, True], [32, True], [32, False]]\r\n",
        "        self.fully_layers = [32]\r\n",
        "        self.lr = 0.001\r\n",
        "        self.log_dir = './logs/'  # tensorboard日志文件\r\n",
        "\r\n",
        "        self.model_file = '{}{}.h5'.format('./model/', data_name)  # lstm模型文件\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def load_data(fname):\r\n",
        "        \"\"\"\r\n",
        "        加载数据。\r\n",
        "        :param fname: 数据集文件名\r\n",
        "        :return: dataframe\r\n",
        "        \"\"\"\r\n",
        "        if os.path.exists(fname):\r\n",
        "            print(\"开始加载编码后的密码数据：%s\" % fname)\r\n",
        "            return pd.read_csv(fname)['grammar']\r\n",
        "        else:\r\n",
        "            logger.error(\"文件不存在：%s\" % fname)\r\n",
        "            sys.exit(1)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def load_tokenizer(fname):\r\n",
        "        \"\"\"\r\n",
        "        从文件中加载tokenizer模型\r\n",
        "        :param fname:\r\n",
        "        :return: tokenizer模型\r\n",
        "        \"\"\"\r\n",
        "        if os.path.exists(fname):\r\n",
        "            print(\"开始加载tokenizer模型：%s\" % fname)\r\n",
        "            with open(fname, 'rb') as file:\r\n",
        "                tokenizer = pickle.load(file)\r\n",
        "                return tokenizer\r\n",
        "        else:\r\n",
        "            logger.error(\"tokenizer模型文件不存在：%s\" % fname)\r\n",
        "            sys.exit(1)\r\n",
        "\r\n",
        "    def encode_data(self):\r\n",
        "        \"\"\"\r\n",
        "        将文本转换为（整数）序列\r\n",
        "        :return: None\r\n",
        "        \"\"\"\r\n",
        "        # 将编码后的密码转换为整数序列\r\n",
        "        print(\"将编码后的密码转换为（整数）序列\")\r\n",
        "        sequences = list()\r\n",
        "\r\n",
        "        data = self.data.values.tolist()\r\n",
        "        for line in data:  # 'L8 D1 '\r\n",
        "            line += '<END>'\r\n",
        "            # 将文本转换为（整数）序列\r\n",
        "            encoded = self.tokenizer.texts_to_sequences([line])[0]\r\n",
        "            # 过滤掉长度大于 MAX_SEQ_LEN 的序列\r\n",
        "            if len(encoded) > self.max_length:\r\n",
        "                continue\r\n",
        "            for i in range(1, len(encoded) + 1):\r\n",
        "                sequence = encoded[:i]\r\n",
        "                sequences.append(sequence)\r\n",
        "\r\n",
        "        print('Total Sequences: %d' % len(sequences))\r\n",
        "\r\n",
        "        # pad input sequences\r\n",
        "        self.max_length = max([len(seq) for seq in sequences])\r\n",
        "        sequences = pad_sequences(sequences, self.max_length, padding='pre')  # 左边填充0\r\n",
        "        print('Max Sequence Length: %d' % self.max_length)\r\n",
        "\r\n",
        "        # 创建输入输出\r\n",
        "        print(\"创建LSTM模型的输入输出\")\r\n",
        "        sequences = np.array(sequences)\r\n",
        "        x, y = sequences[:, :-1], sequences[:, -1]\r\n",
        "        print(\"X Shape: %s, y Shape: %s\" % (x.shape, y.shape))\r\n",
        "        y = to_categorical(y, num_classes=self.vocab_size)  # 对输出进行one-hot编码\r\n",
        "\r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def split_data(self):\r\n",
        "        \"\"\"\r\n",
        "        将lstm的输入输出数据划分为训练集、验证集和测试集\r\n",
        "        :return: x_train, x_val, x_test, y_train, y_val, y_test\r\n",
        "        \"\"\"\r\n",
        "        # 将文本转换为（整数）序列\r\n",
        "        x, y = self.encode_data()\r\n",
        "\r\n",
        "        ratio = 0.6  # 训练集比例\r\n",
        "        if len(x) > 100000:\r\n",
        "            ratio = 0.9\r\n",
        "\r\n",
        "        print(\"划分训练集、验证集和测试集\")\r\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1 - ratio, random_state=SEED)\r\n",
        "        x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=SEED)\r\n",
        "\r\n",
        "        print(\"x_train Shape: %s, y_train Shape: %s\" % (x_train.shape, y_train.shape))\r\n",
        "        print(\"x_val Shape: %s, y_val Shape: %s\" % (x_val.shape, y_val.shape))\r\n",
        "        print(\"x_test Shape: %s, y_test Shape: %s\" % (x_test.shape, y_test.shape))\r\n",
        "\r\n",
        "        return x_train, x_val, x_test, y_train, y_val, y_test\r\n",
        "\r\n",
        "    def create_and_train_model(self):\r\n",
        "        \"\"\"\r\n",
        "        创建和训练LSTM模型\r\n",
        "        :return: None\r\n",
        "        \"\"\"\r\n",
        "        # 划分训练集、验证集和测试集\r\n",
        "        x_train, x_val, x_test, y_train, y_val, y_test = self.split_data()\r\n",
        "\r\n",
        "        print('{:*^106}'.format('创建LSTM模型'))\r\n",
        "        model = Sequential()\r\n",
        "        model.add(Embedding(input_dim=self.vocab_size, output_dim=10, input_length=self.max_length - 1))\r\n",
        "\r\n",
        "        for hidden_size, rs in self.lstm_layers:\r\n",
        "            model.add(LSTM(hidden_size, return_sequences=rs))\r\n",
        "\r\n",
        "        for hidden_size in self.fully_layers:\r\n",
        "            model.add(Dense(units=hidden_size, activation='relu'))\r\n",
        "        model.add(Dense(self.vocab_size, activation='softmax'))\r\n",
        "\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        adam = optimizers.Adam(lr=self.lr)\r\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
        "\r\n",
        "        cur_time = time.strftime(\"%y%m%d%H%M%S\", time.localtime())\r\n",
        "        log_name = '{}{}{}'.format(self.log_dir, self.data_name, cur_time)\r\n",
        "        tensorboard = TensorBoard(log_dir=log_name)\r\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=50)\r\n",
        "\r\n",
        "        print('{:*^106}'.format('开始训练LSTM模型'))\r\n",
        "        model.fit(x_train, y_train,\r\n",
        "                  validation_data=(x_val, y_val),\r\n",
        "                  epochs=self.epochs,\r\n",
        "                  batch_size=self.batch_size,\r\n",
        "                  verbose=1,\r\n",
        "                  callbacks=[tensorboard, early_stopping])\r\n",
        "        loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "        print(\"Model Accuracy on test: %.2f%%, Loss: %.2f\" % (accuracy * 100, loss))\r\n",
        "\r\n",
        "        print(\"保存模型：%s\" % self.model_file)\r\n",
        "        model.save(self.model_file)\r\n",
        "        print('TensorBoard 日志：{}'.format(log_name))\r\n",
        "        print('{:*^106}'.format('完成训练LSTM模型'))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcvMaCpUxgq_"
      },
      "source": [
        "## 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izqnpKu5xjtp",
        "outputId": "48b517bb-5233-4621-e930-cf7149910037"
      },
      "source": [
        "data_sets = ['myspace', 'phpbb']\r\n",
        "# 使用PCFG编码后的密码训练lstm模型\r\n",
        "for name in data_sets:\r\n",
        "  word_lstm = WordLSTM('/content/', name)\r\n",
        "  word_lstm.create_and_train_model()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始加载编码后的密码数据：/content/myspace.txt\n",
            "开始加载tokenizer模型：/content/myspace.pkl\n",
            "将编码后的密码转换为（整数）序列\n",
            "Total Sequences: 8583\n",
            "Max Sequence Length: 18\n",
            "创建LSTM模型的输入输出\n",
            "X Shape: (8583, 17), y Shape: (8583,)\n",
            "划分训练集、验证集和测试集\n",
            "x_train Shape: (5149, 17), y_train Shape: (5149, 70)\n",
            "x_val Shape: (1717, 17), y_val Shape: (1717, 70)\n",
            "x_test Shape: (1717, 17), y_test Shape: (1717, 70)\n",
            "*************************************************创建LSTM模型*************************************************\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 17, 10)            700       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 17, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 17, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 70)                2310      \n",
            "=================================================================\n",
            "Total params: 26,210\n",
            "Trainable params: 26,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "************************************************开始训练LSTM模型************************************************\n",
            "Epoch 1/200\n",
            "41/41 [==============================] - 8s 75ms/step - loss: 4.0363 - accuracy: 0.1785 - val_loss: 2.8891 - val_accuracy: 0.2114\n",
            "Epoch 2/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.7411 - accuracy: 0.2057 - val_loss: 2.6817 - val_accuracy: 0.2114\n",
            "Epoch 3/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6408 - accuracy: 0.1934 - val_loss: 2.6590 - val_accuracy: 0.2114\n",
            "Epoch 4/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6215 - accuracy: 0.2040 - val_loss: 2.6527 - val_accuracy: 0.2114\n",
            "Epoch 5/200\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 2.6212 - accuracy: 0.1860 - val_loss: 2.6489 - val_accuracy: 0.2114\n",
            "Epoch 6/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6322 - accuracy: 0.2017 - val_loss: 2.6503 - val_accuracy: 0.2114\n",
            "Epoch 7/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6124 - accuracy: 0.1877 - val_loss: 2.6415 - val_accuracy: 0.2114\n",
            "Epoch 8/200\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 2.6232 - accuracy: 0.1908 - val_loss: 2.6455 - val_accuracy: 0.2114\n",
            "Epoch 9/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6261 - accuracy: 0.2014 - val_loss: 2.6507 - val_accuracy: 0.2114\n",
            "Epoch 10/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6246 - accuracy: 0.2002 - val_loss: 2.6463 - val_accuracy: 0.2114\n",
            "Epoch 11/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6189 - accuracy: 0.1952 - val_loss: 2.6441 - val_accuracy: 0.2114\n",
            "Epoch 12/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6189 - accuracy: 0.1933 - val_loss: 2.6456 - val_accuracy: 0.2114\n",
            "Epoch 13/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6188 - accuracy: 0.1941 - val_loss: 2.6463 - val_accuracy: 0.2114\n",
            "Epoch 14/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.6325 - accuracy: 0.1920 - val_loss: 2.6472 - val_accuracy: 0.2114\n",
            "Epoch 15/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6330 - accuracy: 0.1947 - val_loss: 2.6451 - val_accuracy: 0.2114\n",
            "Epoch 16/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6249 - accuracy: 0.2013 - val_loss: 2.6489 - val_accuracy: 0.2114\n",
            "Epoch 17/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6251 - accuracy: 0.1895 - val_loss: 2.6450 - val_accuracy: 0.2114\n",
            "Epoch 18/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.6551 - accuracy: 0.1937 - val_loss: 2.6473 - val_accuracy: 0.2114\n",
            "Epoch 19/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6385 - accuracy: 0.1914 - val_loss: 2.6499 - val_accuracy: 0.2114\n",
            "Epoch 20/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.6350 - accuracy: 0.1907 - val_loss: 2.6433 - val_accuracy: 0.2114\n",
            "Epoch 21/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.6317 - accuracy: 0.1945 - val_loss: 2.6417 - val_accuracy: 0.2114\n",
            "Epoch 22/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6185 - accuracy: 0.1969 - val_loss: 2.6306 - val_accuracy: 0.2114\n",
            "Epoch 23/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.6252 - accuracy: 0.1926 - val_loss: 2.5685 - val_accuracy: 0.2568\n",
            "Epoch 24/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.5279 - accuracy: 0.2365 - val_loss: 2.5133 - val_accuracy: 0.2586\n",
            "Epoch 25/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.4956 - accuracy: 0.2431 - val_loss: 2.4528 - val_accuracy: 0.2551\n",
            "Epoch 26/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.3891 - accuracy: 0.2664 - val_loss: 2.3822 - val_accuracy: 0.2784\n",
            "Epoch 27/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.3290 - accuracy: 0.3023 - val_loss: 2.3097 - val_accuracy: 0.3058\n",
            "Epoch 28/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.2335 - accuracy: 0.3077 - val_loss: 2.2459 - val_accuracy: 0.3221\n",
            "Epoch 29/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.2242 - accuracy: 0.3184 - val_loss: 2.2148 - val_accuracy: 0.3197\n",
            "Epoch 30/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.1789 - accuracy: 0.3314 - val_loss: 2.2053 - val_accuracy: 0.3034\n",
            "Epoch 31/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.1513 - accuracy: 0.3277 - val_loss: 2.1698 - val_accuracy: 0.3128\n",
            "Epoch 32/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.1328 - accuracy: 0.3206 - val_loss: 2.1523 - val_accuracy: 0.3262\n",
            "Epoch 33/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 2.1063 - accuracy: 0.3450 - val_loss: 2.1415 - val_accuracy: 0.3128\n",
            "Epoch 34/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.1128 - accuracy: 0.3264 - val_loss: 2.1327 - val_accuracy: 0.3250\n",
            "Epoch 35/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.1118 - accuracy: 0.3363 - val_loss: 2.1276 - val_accuracy: 0.3192\n",
            "Epoch 36/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0632 - accuracy: 0.3478 - val_loss: 2.1221 - val_accuracy: 0.3337\n",
            "Epoch 37/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0985 - accuracy: 0.3316 - val_loss: 2.1142 - val_accuracy: 0.3326\n",
            "Epoch 38/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.1038 - accuracy: 0.3383 - val_loss: 2.1138 - val_accuracy: 0.3308\n",
            "Epoch 39/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0645 - accuracy: 0.3496 - val_loss: 2.1008 - val_accuracy: 0.3343\n",
            "Epoch 40/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0632 - accuracy: 0.3559 - val_loss: 2.0974 - val_accuracy: 0.3308\n",
            "Epoch 41/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0499 - accuracy: 0.3459 - val_loss: 2.0930 - val_accuracy: 0.3349\n",
            "Epoch 42/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0462 - accuracy: 0.3543 - val_loss: 2.0951 - val_accuracy: 0.3349\n",
            "Epoch 43/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0579 - accuracy: 0.3407 - val_loss: 2.0895 - val_accuracy: 0.3326\n",
            "Epoch 44/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0555 - accuracy: 0.3507 - val_loss: 2.0828 - val_accuracy: 0.3366\n",
            "Epoch 45/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0544 - accuracy: 0.3465 - val_loss: 2.0820 - val_accuracy: 0.3366\n",
            "Epoch 46/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 2.0178 - accuracy: 0.3608 - val_loss: 2.0816 - val_accuracy: 0.3349\n",
            "Epoch 47/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0292 - accuracy: 0.3502 - val_loss: 2.0785 - val_accuracy: 0.3390\n",
            "Epoch 48/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0381 - accuracy: 0.3587 - val_loss: 2.0826 - val_accuracy: 0.3349\n",
            "Epoch 49/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9946 - accuracy: 0.3626 - val_loss: 2.0778 - val_accuracy: 0.3401\n",
            "Epoch 50/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9738 - accuracy: 0.3700 - val_loss: 2.0728 - val_accuracy: 0.3553\n",
            "Epoch 51/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0065 - accuracy: 0.3617 - val_loss: 2.0862 - val_accuracy: 0.3366\n",
            "Epoch 52/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0223 - accuracy: 0.3619 - val_loss: 2.0709 - val_accuracy: 0.3448\n",
            "Epoch 53/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9863 - accuracy: 0.3578 - val_loss: 2.0725 - val_accuracy: 0.3518\n",
            "Epoch 54/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 2.0005 - accuracy: 0.3708 - val_loss: 2.0636 - val_accuracy: 0.3640\n",
            "Epoch 55/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9905 - accuracy: 0.3700 - val_loss: 2.0631 - val_accuracy: 0.3628\n",
            "Epoch 56/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9986 - accuracy: 0.3731 - val_loss: 2.0634 - val_accuracy: 0.3582\n",
            "Epoch 57/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9828 - accuracy: 0.3796 - val_loss: 2.0579 - val_accuracy: 0.3663\n",
            "Epoch 58/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9729 - accuracy: 0.3933 - val_loss: 2.0591 - val_accuracy: 0.3599\n",
            "Epoch 59/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 2.0035 - accuracy: 0.3706 - val_loss: 2.0592 - val_accuracy: 0.3698\n",
            "Epoch 60/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9852 - accuracy: 0.3898 - val_loss: 2.0572 - val_accuracy: 0.3588\n",
            "Epoch 61/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9605 - accuracy: 0.3844 - val_loss: 2.0571 - val_accuracy: 0.3640\n",
            "Epoch 62/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9893 - accuracy: 0.3733 - val_loss: 2.0492 - val_accuracy: 0.3623\n",
            "Epoch 63/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9629 - accuracy: 0.3910 - val_loss: 2.0524 - val_accuracy: 0.3704\n",
            "Epoch 64/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9728 - accuracy: 0.3810 - val_loss: 2.0486 - val_accuracy: 0.3605\n",
            "Epoch 65/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9998 - accuracy: 0.3742 - val_loss: 2.0606 - val_accuracy: 0.3658\n",
            "Epoch 66/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9614 - accuracy: 0.3942 - val_loss: 2.0464 - val_accuracy: 0.3687\n",
            "Epoch 67/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9631 - accuracy: 0.3848 - val_loss: 2.0463 - val_accuracy: 0.3628\n",
            "Epoch 68/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9486 - accuracy: 0.3976 - val_loss: 2.0450 - val_accuracy: 0.3628\n",
            "Epoch 69/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9714 - accuracy: 0.3855 - val_loss: 2.0425 - val_accuracy: 0.3646\n",
            "Epoch 70/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9847 - accuracy: 0.3857 - val_loss: 2.0440 - val_accuracy: 0.3605\n",
            "Epoch 71/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9645 - accuracy: 0.3891 - val_loss: 2.0347 - val_accuracy: 0.3687\n",
            "Epoch 72/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9513 - accuracy: 0.3937 - val_loss: 2.0407 - val_accuracy: 0.3739\n",
            "Epoch 73/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9367 - accuracy: 0.3958 - val_loss: 2.0382 - val_accuracy: 0.3669\n",
            "Epoch 74/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9313 - accuracy: 0.3968 - val_loss: 2.0444 - val_accuracy: 0.3692\n",
            "Epoch 75/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9757 - accuracy: 0.3837 - val_loss: 2.0494 - val_accuracy: 0.3663\n",
            "Epoch 76/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9314 - accuracy: 0.3900 - val_loss: 2.0316 - val_accuracy: 0.3722\n",
            "Epoch 77/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9467 - accuracy: 0.3922 - val_loss: 2.0329 - val_accuracy: 0.3675\n",
            "Epoch 78/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9589 - accuracy: 0.3897 - val_loss: 2.0372 - val_accuracy: 0.3687\n",
            "Epoch 79/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9624 - accuracy: 0.3862 - val_loss: 2.0348 - val_accuracy: 0.3628\n",
            "Epoch 80/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9332 - accuracy: 0.3971 - val_loss: 2.0503 - val_accuracy: 0.3588\n",
            "Epoch 81/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9781 - accuracy: 0.3811 - val_loss: 2.0272 - val_accuracy: 0.3710\n",
            "Epoch 82/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9346 - accuracy: 0.3937 - val_loss: 2.0261 - val_accuracy: 0.3692\n",
            "Epoch 83/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9635 - accuracy: 0.3864 - val_loss: 2.0321 - val_accuracy: 0.3669\n",
            "Epoch 84/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9431 - accuracy: 0.3840 - val_loss: 2.0307 - val_accuracy: 0.3710\n",
            "Epoch 85/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9190 - accuracy: 0.3927 - val_loss: 2.0247 - val_accuracy: 0.3704\n",
            "Epoch 86/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9412 - accuracy: 0.3941 - val_loss: 2.0206 - val_accuracy: 0.3710\n",
            "Epoch 87/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9296 - accuracy: 0.4005 - val_loss: 2.0142 - val_accuracy: 0.3768\n",
            "Epoch 88/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9492 - accuracy: 0.3844 - val_loss: 2.0212 - val_accuracy: 0.3733\n",
            "Epoch 89/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9397 - accuracy: 0.3933 - val_loss: 2.0263 - val_accuracy: 0.3757\n",
            "Epoch 90/200\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.9375 - accuracy: 0.3775 - val_loss: 2.0150 - val_accuracy: 0.3768\n",
            "Epoch 91/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9332 - accuracy: 0.3939 - val_loss: 2.0298 - val_accuracy: 0.3722\n",
            "Epoch 92/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9036 - accuracy: 0.3976 - val_loss: 2.0123 - val_accuracy: 0.3774\n",
            "Epoch 93/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9290 - accuracy: 0.3921 - val_loss: 2.0127 - val_accuracy: 0.3716\n",
            "Epoch 94/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9246 - accuracy: 0.3975 - val_loss: 2.0111 - val_accuracy: 0.3809\n",
            "Epoch 95/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9302 - accuracy: 0.3918 - val_loss: 2.0129 - val_accuracy: 0.3791\n",
            "Epoch 96/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9187 - accuracy: 0.3900 - val_loss: 2.0166 - val_accuracy: 0.3768\n",
            "Epoch 97/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9095 - accuracy: 0.3922 - val_loss: 2.0115 - val_accuracy: 0.3733\n",
            "Epoch 98/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8952 - accuracy: 0.3948 - val_loss: 2.0218 - val_accuracy: 0.3768\n",
            "Epoch 99/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9043 - accuracy: 0.3931 - val_loss: 2.0075 - val_accuracy: 0.3692\n",
            "Epoch 100/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9602 - accuracy: 0.3869 - val_loss: 2.0081 - val_accuracy: 0.3797\n",
            "Epoch 101/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9227 - accuracy: 0.3934 - val_loss: 2.0110 - val_accuracy: 0.3774\n",
            "Epoch 102/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8718 - accuracy: 0.4066 - val_loss: 2.0116 - val_accuracy: 0.3739\n",
            "Epoch 103/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9361 - accuracy: 0.3825 - val_loss: 2.0104 - val_accuracy: 0.3716\n",
            "Epoch 104/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9025 - accuracy: 0.3940 - val_loss: 2.0180 - val_accuracy: 0.3762\n",
            "Epoch 105/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8953 - accuracy: 0.4053 - val_loss: 2.0074 - val_accuracy: 0.3780\n",
            "Epoch 106/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8868 - accuracy: 0.4038 - val_loss: 2.0082 - val_accuracy: 0.3716\n",
            "Epoch 107/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9370 - accuracy: 0.3811 - val_loss: 2.0184 - val_accuracy: 0.3692\n",
            "Epoch 108/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8949 - accuracy: 0.3975 - val_loss: 2.0051 - val_accuracy: 0.3780\n",
            "Epoch 109/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8756 - accuracy: 0.3947 - val_loss: 2.0059 - val_accuracy: 0.3733\n",
            "Epoch 110/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8453 - accuracy: 0.4047 - val_loss: 2.0115 - val_accuracy: 0.3791\n",
            "Epoch 111/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8943 - accuracy: 0.4024 - val_loss: 2.0016 - val_accuracy: 0.3774\n",
            "Epoch 112/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9161 - accuracy: 0.3952 - val_loss: 2.0110 - val_accuracy: 0.3733\n",
            "Epoch 113/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.9088 - accuracy: 0.3836 - val_loss: 2.0088 - val_accuracy: 0.3797\n",
            "Epoch 114/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8990 - accuracy: 0.3992 - val_loss: 2.0032 - val_accuracy: 0.3751\n",
            "Epoch 115/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8648 - accuracy: 0.3984 - val_loss: 2.0080 - val_accuracy: 0.3774\n",
            "Epoch 116/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8851 - accuracy: 0.3975 - val_loss: 2.0062 - val_accuracy: 0.3821\n",
            "Epoch 117/200\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.8813 - accuracy: 0.4053 - val_loss: 2.0123 - val_accuracy: 0.3867\n",
            "Epoch 118/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8934 - accuracy: 0.3934 - val_loss: 2.0060 - val_accuracy: 0.3832\n",
            "Epoch 119/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8878 - accuracy: 0.3930 - val_loss: 2.0125 - val_accuracy: 0.3821\n",
            "Epoch 120/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8978 - accuracy: 0.3898 - val_loss: 2.0140 - val_accuracy: 0.3885\n",
            "Epoch 121/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8913 - accuracy: 0.4051 - val_loss: 2.0130 - val_accuracy: 0.3844\n",
            "Epoch 122/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8911 - accuracy: 0.3935 - val_loss: 2.0158 - val_accuracy: 0.3873\n",
            "Epoch 123/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9111 - accuracy: 0.3936 - val_loss: 2.0104 - val_accuracy: 0.3832\n",
            "Epoch 124/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8664 - accuracy: 0.4072 - val_loss: 2.0164 - val_accuracy: 0.3867\n",
            "Epoch 125/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8880 - accuracy: 0.4025 - val_loss: 2.0100 - val_accuracy: 0.3861\n",
            "Epoch 126/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9117 - accuracy: 0.3939 - val_loss: 2.0213 - val_accuracy: 0.3768\n",
            "Epoch 127/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8990 - accuracy: 0.3937 - val_loss: 2.0074 - val_accuracy: 0.3856\n",
            "Epoch 128/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8638 - accuracy: 0.4085 - val_loss: 2.0116 - val_accuracy: 0.3844\n",
            "Epoch 129/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8731 - accuracy: 0.3974 - val_loss: 2.0104 - val_accuracy: 0.3826\n",
            "Epoch 130/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8802 - accuracy: 0.4055 - val_loss: 2.0108 - val_accuracy: 0.3844\n",
            "Epoch 131/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.9190 - accuracy: 0.3863 - val_loss: 2.0221 - val_accuracy: 0.3844\n",
            "Epoch 132/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8999 - accuracy: 0.3956 - val_loss: 2.0149 - val_accuracy: 0.3879\n",
            "Epoch 133/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8644 - accuracy: 0.3997 - val_loss: 2.0099 - val_accuracy: 0.3838\n",
            "Epoch 134/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8636 - accuracy: 0.4036 - val_loss: 2.0139 - val_accuracy: 0.3891\n",
            "Epoch 135/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8674 - accuracy: 0.4004 - val_loss: 2.0135 - val_accuracy: 0.3838\n",
            "Epoch 136/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8647 - accuracy: 0.4055 - val_loss: 2.0198 - val_accuracy: 0.3815\n",
            "Epoch 137/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8740 - accuracy: 0.3980 - val_loss: 2.0145 - val_accuracy: 0.3896\n",
            "Epoch 138/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8824 - accuracy: 0.3984 - val_loss: 2.0163 - val_accuracy: 0.3885\n",
            "Epoch 139/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8714 - accuracy: 0.3969 - val_loss: 2.0136 - val_accuracy: 0.3867\n",
            "Epoch 140/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8555 - accuracy: 0.3977 - val_loss: 2.0273 - val_accuracy: 0.3856\n",
            "Epoch 141/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8916 - accuracy: 0.3928 - val_loss: 2.0172 - val_accuracy: 0.3809\n",
            "Epoch 142/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8649 - accuracy: 0.4025 - val_loss: 2.0168 - val_accuracy: 0.3885\n",
            "Epoch 143/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8861 - accuracy: 0.3941 - val_loss: 2.0139 - val_accuracy: 0.3832\n",
            "Epoch 144/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8625 - accuracy: 0.3984 - val_loss: 2.0152 - val_accuracy: 0.3885\n",
            "Epoch 145/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8683 - accuracy: 0.4038 - val_loss: 2.0218 - val_accuracy: 0.3838\n",
            "Epoch 146/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8587 - accuracy: 0.4121 - val_loss: 2.0189 - val_accuracy: 0.3844\n",
            "Epoch 147/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8530 - accuracy: 0.4042 - val_loss: 2.0174 - val_accuracy: 0.3815\n",
            "Epoch 148/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8520 - accuracy: 0.4026 - val_loss: 2.0167 - val_accuracy: 0.3856\n",
            "Epoch 149/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8658 - accuracy: 0.3983 - val_loss: 2.0209 - val_accuracy: 0.3821\n",
            "Epoch 150/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8591 - accuracy: 0.3972 - val_loss: 2.0164 - val_accuracy: 0.3838\n",
            "Epoch 151/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8409 - accuracy: 0.4008 - val_loss: 2.0199 - val_accuracy: 0.3867\n",
            "Epoch 152/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8252 - accuracy: 0.4084 - val_loss: 2.0150 - val_accuracy: 0.3902\n",
            "Epoch 153/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8743 - accuracy: 0.4007 - val_loss: 2.0189 - val_accuracy: 0.3826\n",
            "Epoch 154/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8771 - accuracy: 0.4052 - val_loss: 2.0206 - val_accuracy: 0.3867\n",
            "Epoch 155/200\n",
            "41/41 [==============================] - 1s 34ms/step - loss: 1.8727 - accuracy: 0.3990 - val_loss: 2.0171 - val_accuracy: 0.3856\n",
            "Epoch 156/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8600 - accuracy: 0.3979 - val_loss: 2.0196 - val_accuracy: 0.3856\n",
            "Epoch 157/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8689 - accuracy: 0.3982 - val_loss: 2.0212 - val_accuracy: 0.3844\n",
            "Epoch 158/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8721 - accuracy: 0.4001 - val_loss: 2.0214 - val_accuracy: 0.3844\n",
            "Epoch 159/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8644 - accuracy: 0.4060 - val_loss: 2.0294 - val_accuracy: 0.3856\n",
            "Epoch 160/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8632 - accuracy: 0.3991 - val_loss: 2.0269 - val_accuracy: 0.3856\n",
            "Epoch 161/200\n",
            "41/41 [==============================] - 1s 35ms/step - loss: 1.8514 - accuracy: 0.4101 - val_loss: 2.0247 - val_accuracy: 0.3832\n",
            "Model Accuracy on test: 38.15%, Loss: 1.96\n",
            "保存模型：./model/myspace.h5\n",
            "TensorBoard 日志：./logs/myspace210308041759\n",
            "************************************************完成训练LSTM模型************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGCOWm_2Exfk"
      },
      "source": [
        "## acc-loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqWIYlsDE1bQ"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir '/content/logs/phpbb210308040234'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}